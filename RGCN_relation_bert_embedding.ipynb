{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb737485-13df-48db-83c3-bb85ba4ae026",
   "metadata": {},
   "source": [
    "# RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d9360e-3b70-4f40-bd1e-965dd3c9f67c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='FB15k-237', dropout=0.2, emb_type='bert', evaluate_every=10, f='/home/roy206/.local/share/jupyter/runtime/kernel-3578a416-edee-49c1-ab3a-2347c8e30dfa.json', gpu=0, grad_norm=1.0, graph_batch_size=1024, graph_split_size=0.5, hid_dim=128, long_text=False, lr=0.01, max_relations=-1, n_bases=4, n_epochs=1, negative_sample=1, regularization=0.01, test=True)\n",
      "cuda\n",
      "load data from ./data/FB15k-237\n",
      "num_entity: 14541\n",
      "num_relation: 237\n",
      "num_train_triples: 272115\n",
      "num_valid_triples: 17535\n",
      "num_test_triples: 20466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded BERT\n",
      "Obtained texts\n",
      "Obtained encodings\n",
      "Successfully obtained embedding from BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20466, 3])\n",
      "torch.Size([310116, 3])\n",
      "Everyting in GPU\n",
      "Obtained texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<01:08, 14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained encodings\n",
      "Successfully obtained embedding from BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (filtered): 0.001092\n",
      "Hits (filtered) @ 1: 0.000500\n",
      "Hits (filtered) @ 3: 0.000500\n",
      "Hits (filtered) @ 10: 0.001500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # RGCN\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "\n",
    "# from utils import uniform\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, id2entity, id2relation, num_bases, dropout, device, file_path, emb_type=\"scratch\", hid_dim=768, long_text=False):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        self.emb_type = emb_type\n",
    "        \n",
    "        self.id2entity = id2entity\n",
    "        self.id2relation = id2relation\n",
    "        self.device = device\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        if emb_type == \"scratch\":\n",
    "            self.entity_embedding = nn.Embedding(num_entities, self.hid_dim)\n",
    "            \n",
    "        else:\n",
    "#             self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#             self.bert = BertModel.from_pretrained('bert-base-cased', output_hidden_states=True).to(device)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "            self.bert = BertModel.from_pretrained('prajjwal1/bert-tiny', output_hidden_states=True).to(device)\n",
    "            self.entity2text_dict = pd.read_csv(file_path + \"/entity2textlong.txt\", sep=\"\\t\", header=None, names=[\"id\", \"desc\"], dtype=str).set_index(\"id\").to_dict()['desc'] \\\n",
    "                                    if long_text \\\n",
    "                                    else pd.read_csv(file_path + \"/entity2text.txt\", sep=\"\\t\", header=None, names=[\"id\", \"name\"], dtype=str).set_index(\"id\").to_dict()['name']\n",
    "            #entity2textlong = pd.read_csv(file_path + \"/entity2textlong.txt\", sep=\"\\t\", header=None, names=[\"id\", \"desc\"]).set_index(\"id\")\n",
    "            #entity2text = pd.read_csv(file_path + \"/entity2text.txt\", sep=\"\\t\", header=None, names=[\"id\", \"name\"]).set_index(\"id\")\n",
    "            #self.entity2textlong_dict = entity2textlong.to_dict()['desc']\n",
    "            #self.entity2text_dict = entity2text.to_dict()['name']\n",
    "            \n",
    "            print(\"Successfully Loaded BERT\")\n",
    "            \n",
    "        self.relation_embedding = nn.Parameter(torch.Tensor(num_relations, self.hid_dim)).to(device)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embedding, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.conv1 = RGCNConv(\n",
    "            self.hid_dim, self.hid_dim, num_relations * 2, num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(\n",
    "            self.hid_dim, self.hid_dim, num_relations * 2, num_bases=num_bases)\n",
    "\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "    def forward(self, entity, edge_index, edge_type, edge_norm):\n",
    "        if self.emb_type == \"scratch\":\n",
    "            x = self.entity_embedding(entity)\n",
    "        else:\n",
    "            texts = [self.entity2text_dict[self.id2entity[idx.item()]] for idx in entity]\n",
    "            \n",
    "            print(\"Obtained texts\")\n",
    "            name_last_layers, name_encodings = self.get_model_outputs(texts)\n",
    "            print(\"Obtained encodings\")\n",
    "            x = self.output2embedding(name_last_layers, name_encodings, only_attention_mask=True)\n",
    "            print(\"Successfully obtained embedding from BERT\")\n",
    "            \n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type, edge_norm))\n",
    "        x = F.dropout(x, p = self.dropout_ratio, training = self.training)\n",
    "        # x = self.conv2(x, edge_index, edge_type, edge_norm)\n",
    "        # print(\"train5\")\n",
    "        return x\n",
    "\n",
    "    def distmult(self, embedding, triplets):\n",
    "#         s = embedding[triplets[:,0]]\n",
    "#         r = self.relation_embedding[triplets[:,1]]\n",
    "#         o = embedding[triplets[:,2]]\n",
    "#         score = torch.sum(s * r * o, dim=1)\n",
    "        \n",
    "#         return score\n",
    "\n",
    "        s = embedding[triplets[:,0]]\n",
    "        relations = triplets[:,1]\n",
    "        texts = [self.entity2text_dict[self.id2relation[relation.item()]] for relation in relations]\n",
    "        name_last_layers, name_encodings = self.get_model_outputs(texts)\n",
    "        r = self.output2embedding(name_last_layers, name_encodings, only_attention_mask=True)\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def score_loss(self, embedding, triplets, target):\n",
    "        score = self.distmult(embedding, triplets)\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(score, target)\n",
    "\n",
    "    def reg_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.relation_embedding.pow(2))\n",
    "    \n",
    "    def get_model_outputs(self,texts):\n",
    "      # Use base model\n",
    "\n",
    "      # WARN: Long sentencs are truncated to the first 512 tokens.\n",
    "      # https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification/63413589#63413589\n",
    "      encodings = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "      \n",
    "      for item in encodings:\n",
    "            encodings[item] = encodings[item].to(self.device)\n",
    "     \n",
    "      self.bert = self.bert.to(self.device)\n",
    "        \n",
    "      if args.test:\n",
    "          with torch.no_grad():\n",
    "            last_layers = self.bert(**encodings)      \n",
    "      else:\n",
    "          last_layers = self.bert(**encodings)\n",
    "\n",
    "      \n",
    "      \n",
    "    \n",
    "      # for item in encodings:\n",
    "      #       encodings[item] = encodings[item].cuda()\n",
    "      # last_layers = last_layers.cuda()\n",
    "      return last_layers, encodings\n",
    "\n",
    "    def output2embedding(self,last_layers, encodings, only_attention_mask=False):\n",
    "      # Get an embedding for each sentence by averaging the embeddings of all tokens in the sentence.\n",
    "      # Note:\n",
    "      #   When passed to BERT, each sentence is padded to ensure all the sentences have the same number of tokens,\n",
    "      #   so, the output of BERT includes embeddings for padding elements, which might become noise.\n",
    "      #   https://huggingface.co/docs/transformers/pad_truncation\n",
    "      # \n",
    "      #   To exclude embeddings of padding elements from the calculation of average, set only_attention_mask = True.\n",
    "      if only_attention_mask:\n",
    "        return (last_layers[0] * encodings[\"attention_mask\"][:, :, None].expand(last_layers[0].shape)).sum(1).div(encodings[\"attention_mask\"].sum(1, keepdim=True))\n",
    "      else:\n",
    "        return last_layers[0].mean(1)\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int): Number of bases used for basis-decomposition.\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, num_bases,\n",
    "                 root_weight=True, bias=True, **kwargs):\n",
    "        super(RGCNConv, self).__init__(aggr='mean', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "\n",
    "        self.basis = nn.Parameter(torch.Tensor(num_bases, in_channels, out_channels))\n",
    "        self.att = nn.Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.num_bases * self.in_channels\n",
    "        uniform(size, self.basis)\n",
    "        uniform(size, self.att)\n",
    "        uniform(size, self.root)\n",
    "        uniform(size, self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type, edge_norm=None, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.propagate(edge_index, size=size, x=x, edge_type=edge_type,\n",
    "                              edge_norm=edge_norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_index_j, edge_type, edge_norm):\n",
    "        w = torch.matmul(self.att, self.basis.view(self.num_bases, -1))\n",
    "\n",
    "        # If no node features are given, we implement a simple embedding\n",
    "        # loopkup based on the target node index and its edge type.\n",
    "        if x_j is None:\n",
    "            w = w.view(-1, self.out_channels)\n",
    "            index = edge_type * self.in_channels + edge_index_j\n",
    "            out = torch.index_select(w, 0, index)\n",
    "        else:\n",
    "            w = w.view(self.num_relations, self.in_channels, self.out_channels)\n",
    "            w = torch.index_select(w, 0, edge_type)\n",
    "            out = torch.bmm(x_j.unsqueeze(1), w).squeeze(-2)\n",
    "\n",
    "        return out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        if self.root is not None:\n",
    "            if x is None:\n",
    "                out = aggr_out + self.root\n",
    "            else:\n",
    "                out = aggr_out + torch.matmul(x, self.root)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, num_relations={})'.format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels,\n",
    "            self.num_relations)\n",
    "\n",
    "\n",
    "# # Utils\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "def load_data(file_path, max_relations):\n",
    "    '''\n",
    "        argument:\n",
    "            file_path: ./data/FB15k-237\n",
    "        \n",
    "        return:\n",
    "            entity2id, relation2id, train_triplets, valid_triplets, test_triplets\n",
    "    '''\n",
    "    print(\"load data from {}\".format(file_path))\n",
    "\n",
    "    with open(os.path.join(file_path, 'entities.dict')) as f:\n",
    "        entity2id = dict()\n",
    "        id2entity = {}\n",
    "\n",
    "        for line in f:\n",
    "            # For sampling\n",
    "            #if np.random.rand(1) > 0.8:\n",
    "            #    continue\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "            id2entity[int(eid)] = entity\n",
    "\n",
    "    with open(os.path.join(file_path, 'relations.dict')) as f:\n",
    "        relation2id = dict()\n",
    "        id2relation = {}\n",
    "\n",
    "        for i, line in enumerate(f):\n",
    "            # For relation sampling\n",
    "            if max_relations > -1 and i >= max_relations:\n",
    "                print(f\"Discard relations after {i-1}\")\n",
    "                break\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "            id2relation[int(rid)] = entity\n",
    "\n",
    "    train_triplets = read_triplets(os.path.join(file_path, 'train.txt'), entity2id, relation2id)\n",
    "    valid_triplets = read_triplets(os.path.join(file_path, 'valid.txt'), entity2id, relation2id)\n",
    "    test_triplets = read_triplets(os.path.join(file_path, 'test.txt'), entity2id, relation2id)\n",
    "\n",
    "    print('num_entity: {}'.format(len(entity2id)))\n",
    "    print('num_relation: {}'.format(len(relation2id)))\n",
    "    print('num_train_triples: {}'.format(len(train_triplets)))\n",
    "    print('num_valid_triples: {}'.format(len(valid_triplets)))\n",
    "    print('num_test_triples: {}'.format(len(test_triplets)))\n",
    "\n",
    "    return entity2id, id2entity, relation2id, id2relation, train_triplets, valid_triplets, test_triplets\n",
    "\n",
    "def read_triplets(file_path, entity2id, relation2id):\n",
    "    triplets = []\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            head, relation, tail = line.strip().split('\\t')\n",
    "            # For sampling\n",
    "            if head not in entity2id or tail not in entity2id or relation not in relation2id:\n",
    "                continue\n",
    "            triplets.append((entity2id[head], relation2id[relation], entity2id[tail]))\n",
    "\n",
    "    return np.array(triplets)\n",
    "\n",
    "def sample_edge_uniform(n_triples, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triples)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.choice(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "def edge_normalization(edge_type, edge_index, num_entity, num_relation):\n",
    "    '''\n",
    "        Edge normalization trick\n",
    "        - one_hot: (num_edge, num_relation)\n",
    "        - deg: (num_node, num_relation)\n",
    "        - index: (num_edge)\n",
    "        - deg[edge_index[0]]: (num_edge, num_relation)\n",
    "        - edge_norm: (num_edge)\n",
    "    '''\n",
    "    one_hot = F.one_hot(edge_type, num_classes = 2 * num_relation).to(torch.float)\n",
    "    deg = scatter_add(one_hot, edge_index[0], dim = 0, dim_size = num_entity)\n",
    "    index = edge_type + torch.arange(len(edge_index[0])) * (2 * num_relation)\n",
    "    edge_norm = 1 / deg[edge_index[0]].view(-1)[index]\n",
    "\n",
    "    return edge_norm\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size, num_entity, num_rels, negative_rate):\n",
    "    \"\"\"\n",
    "        Get training graph and signals\n",
    "        First perform edge neighborhood sampling on graph, then perform negative\n",
    "        sampling to generate negative samples\n",
    "    \"\"\"\n",
    "\n",
    "    edges = sample_edge_uniform(len(triplets), sample_size)\n",
    "\n",
    "    # Select sampled edges\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_entity, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # Negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_entity), negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "\n",
    "    src = torch.tensor(src[graph_split_ids], dtype = torch.long).contiguous()\n",
    "    dst = torch.tensor(dst[graph_split_ids], dtype = torch.long).contiguous()\n",
    "    rel = torch.tensor(rel[graph_split_ids], dtype = torch.long).contiguous()\n",
    "\n",
    "    # Create bi-directional graph\n",
    "    src, dst = torch.cat((src, dst)), torch.cat((dst, src))\n",
    "    rel = torch.cat((rel, rel + num_rels))\n",
    "\n",
    "    edge_index = torch.stack((src, dst))\n",
    "    edge_type = rel\n",
    "\n",
    "    data = Data(edge_index = edge_index)\n",
    "    data.entity = torch.from_numpy(uniq_entity)\n",
    "    data.edge_type = edge_type\n",
    "    data.edge_norm = edge_normalization(edge_type, edge_index, len(uniq_entity), num_rels)\n",
    "    data.samples = torch.from_numpy(samples)\n",
    "    data.labels = torch.from_numpy(labels)\n",
    "\n",
    "    return data\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, triplets):\n",
    "    src, rel, dst = triplets.transpose()\n",
    "\n",
    "    src = torch.from_numpy(src)\n",
    "    rel = torch.from_numpy(rel)\n",
    "    dst = torch.from_numpy(dst)\n",
    "\n",
    "    src, dst = torch.cat((src, dst)), torch.cat((dst, src))\n",
    "    rel = torch.cat((rel, rel + num_rels))\n",
    "    \n",
    "    \n",
    "    edge_index = torch.stack((src, dst))\n",
    "    \n",
    "    \n",
    "    edge_type = rel\n",
    "\n",
    "    data = Data(edge_index = edge_index)\n",
    "    data.entity = torch.from_numpy(np.arange(num_nodes))\n",
    "    data.edge_type = edge_type\n",
    "    data.edge_norm = edge_normalization(edge_type, edge_index, num_nodes, num_rels)\n",
    "\n",
    "    return data\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "# return MRR (filtered), and Hits @ (1, 3, 10)\n",
    "def calc_mrr(embedding, w, test_triplets, all_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        num_entity = len(embedding)\n",
    "\n",
    "        ranks_s = []\n",
    "        ranks_o = []\n",
    "\n",
    "        head_relation_triplets = all_triplets[:, :2]\n",
    "        tail_relation_triplets = torch.stack((all_triplets[:, 2], all_triplets[:, 1])).transpose(0, 1)\n",
    "\n",
    "        for test_triplet in tqdm(test_triplets):\n",
    "\n",
    "            # Perturb object\n",
    "            subject = test_triplet[0]\n",
    "            relation = test_triplet[1]\n",
    "            object_ = test_triplet[2]\n",
    "\n",
    "            subject_relation = test_triplet[:2]\n",
    "            delete_index = torch.sum(head_relation_triplets == subject_relation, dim = 1)\n",
    "            delete_index = torch.nonzero(delete_index == 2).squeeze()\n",
    "\n",
    "            delete_entity_index = all_triplets[delete_index, 2].view(-1).numpy()\n",
    "            perturb_entity_index = np.array(list(set(np.arange(num_entity)) - set(delete_entity_index)))\n",
    "            perturb_entity_index = torch.from_numpy(perturb_entity_index)\n",
    "            perturb_entity_index = torch.cat((perturb_entity_index, object_.view(-1)))\n",
    "            \n",
    "            emb_ar = embedding[subject] * w[relation]\n",
    "            emb_ar = emb_ar.view(-1, 1, 1)\n",
    "\n",
    "            emb_c = embedding[perturb_entity_index]\n",
    "            emb_c = emb_c.transpose(0, 1).unsqueeze(1)\n",
    "            \n",
    "            out_prod = torch.bmm(emb_ar, emb_c)\n",
    "            score = torch.sum(out_prod, dim = 0)\n",
    "            score = torch.sigmoid(score)\n",
    "            \n",
    "            target = torch.tensor(len(perturb_entity_index) - 1)\n",
    "            ranks_s.append(sort_and_rank(score, target))\n",
    "\n",
    "            # Perturb subject\n",
    "            object_ = test_triplet[2]\n",
    "            relation = test_triplet[1]\n",
    "            subject = test_triplet[0]\n",
    "\n",
    "            object_relation = torch.tensor([object_, relation])\n",
    "            delete_index = torch.sum(tail_relation_triplets == object_relation, dim = 1)\n",
    "            delete_index = torch.nonzero(delete_index == 2).squeeze()\n",
    "\n",
    "            delete_entity_index = all_triplets[delete_index, 0].view(-1).numpy()\n",
    "            perturb_entity_index = np.array(list(set(np.arange(num_entity)) - set(delete_entity_index)))\n",
    "            perturb_entity_index = torch.from_numpy(perturb_entity_index)\n",
    "            perturb_entity_index = torch.cat((perturb_entity_index, subject.view(-1)))\n",
    "\n",
    "            emb_ar = embedding[object_] * w[relation]\n",
    "            emb_ar = emb_ar.view(-1, 1, 1)\n",
    "\n",
    "            emb_c = embedding[perturb_entity_index]\n",
    "            emb_c = emb_c.transpose(0, 1).unsqueeze(1)\n",
    "\n",
    "            out_prod = torch.bmm(emb_ar, emb_c)\n",
    "            score = torch.sum(out_prod, dim = 0)\n",
    "            score = torch.sigmoid(score)\n",
    "\n",
    "            target = torch.tensor(len(perturb_entity_index) - 1)\n",
    "            ranks_o.append(sort_and_rank(score, target))\n",
    "\n",
    "        ranks_s = torch.cat(ranks_s)\n",
    "        ranks_o = torch.cat(ranks_o)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "            \n",
    "    return mrr.item()\n",
    "\n",
    "\n",
    "# # Main\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# from utils import load_data, generate_sampled_graph_and_labels, build_test_graph, calc_mrr\n",
    "# from models import RGCN\n",
    "\n",
    "def train(train_triplets, model, use_cuda, batch_size, split_size, negative_sample, reg_ratio, num_entities, num_relations):\n",
    "\n",
    "    train_data = generate_sampled_graph_and_labels(train_triplets, batch_size, split_size, num_entities, num_relations, negative_sample)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "        train_data.to(device)\n",
    "\n",
    "    entity_embedding = model(train_data.entity, train_data.edge_index, train_data.edge_type, train_data.edge_norm)\n",
    "    loss = model.score_loss(entity_embedding, train_data.samples, train_data.labels) + reg_ratio * model.reg_loss(entity_embedding)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def valid(valid_triplets, model, test_graph, all_triplets):\n",
    "\n",
    "    \n",
    "    entity_embedding = model(test_graph.entity, test_graph.edge_index, test_graph.edge_type, test_graph.edge_norm)\n",
    "    mrr = calc_mrr(entity_embedding, model.relation_embedding, valid_triplets, all_triplets, hits=[1, 3, 10])\n",
    "    return mrr\n",
    "\n",
    "def test(test_triplets, model, test_graph, all_triplets):\n",
    "\n",
    "    entity_embedding = model(test_graph.entity, test_graph.edge_index, test_graph.edge_type, test_graph.edge_norm)\n",
    "    mrr = calc_mrr(entity_embedding.cpu(), model.relation_embedding.cpu(), test_triplets.cpu(), all_triplets.cpu(), hits=[1, 3, 10])\n",
    "    return mrr\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    use_cuda = args.gpu >= 0 and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    print(device)\n",
    "    best_mrr = 0\n",
    "    \n",
    "    file_path = './data/' + args.dataset \n",
    "    \n",
    "    entity2id, id2entity, relation2id, id2relation, train_triplets, valid_triplets, test_triplets = load_data(file_path, args.max_relations)\n",
    "    all_triplets = torch.LongTensor(np.concatenate((train_triplets, valid_triplets, test_triplets))).to(device)\n",
    "\n",
    "    \n",
    "    valid_triplets = torch.LongTensor(valid_triplets).to(device)\n",
    "    test_triplets = torch.LongTensor(test_triplets).to(device)\n",
    "\n",
    "    model = RGCN(\n",
    "        len(entity2id), len(relation2id), id2entity, id2relation,\n",
    "        num_bases=args.n_bases, dropout=args.dropout,\n",
    "        device=device, file_path=file_path,\n",
    "        emb_type=args.emb_type, hid_dim=args.hid_dim, long_text=args.long_text\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    model = model.to(device)\n",
    "    # print(model)\n",
    "\n",
    "    # if use_cuda:\n",
    "    #     model.cuda()\n",
    "    \n",
    "    mx_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in trange(1, (args.n_epochs + 1), desc='Epochs', position=0):\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = train(train_triplets, model, use_cuda, batch_size=args.graph_batch_size, split_size=args.graph_split_size, \n",
    "            negative_sample=args.negative_sample, reg_ratio = args.regularization, num_entities=len(entity2id), num_relations=len(relation2id))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if loss < mx_loss:\n",
    "            loss = mx_loss\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},'best_mrr_model.pth')\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "#             print(\"debug0\")\n",
    "            \n",
    "#             tqdm.write(\"Train Loss {} at epoch {}\".format(loss, epoch))\n",
    "\n",
    "#             # if use_cuda:\n",
    "#             #     model.cpu()\n",
    "                \n",
    "#             print(\"debug1\")\n",
    "\n",
    "#             model.eval()\n",
    "#             valid_mrr = valid(valid_triplets, model, test_graph, all_triplets)\n",
    "            \n",
    "#             print(valid_mrr)\n",
    "            \n",
    "#             if valid_mrr > best_mrr:\n",
    "#                 best_mrr = valid_mrr\n",
    "#                 torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "#                             'best_mrr_model.pth')\n",
    "               \n",
    "#             print(\"debug2\")\n",
    "                \n",
    "\n",
    "#             if use_cuda:\n",
    "#                 model.cuda()\n",
    "    \n",
    "#     if use_cuda:\n",
    "#         model.cuda()\n",
    "\n",
    "#     model.eval()\n",
    "#     print(model)\n",
    "    \n",
    "\n",
    "    checkpoint = torch.load('best_mrr_model.pth')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    \n",
    "    test_triplets = test_triplets.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_triplets = all_triplets.to(device)\n",
    "    \n",
    "    print(test_triplets.shape)\n",
    "    print(all_triplets.shape)\n",
    "    \n",
    "    print(\"Everyting in GPU\")\n",
    "    model = model.eval()\n",
    "    \n",
    "    test_graph = build_test_graph(len(entity2id), len(relation2id), train_triplets[:1000,:])\n",
    "    test_graph = test_graph.to(device)\n",
    "    #test_graph = build_test_graph(len(entity2id), len(relation2id), train_triplets[:10000,:])\n",
    "#     test_graph = build_test_graph(len(entity2id), len(relation2id), train_triplets)\n",
    "    test(test_triplets[:1000], model, test_graph, all_triplets[:1000])\n",
    "    # test(test_triplets, model, test_graph, all_triplets)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='RGCN')\n",
    "    \n",
    "    parser.add_argument(\"-f\")\n",
    "    parser.add_argument(\"--test\", type=bool, default=True)\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"FB15k-237\")\n",
    "    parser.add_argument(\"--emb_type\", type=str, default=\"bert\")\n",
    "    parser.add_argument(\"--long_text\", action=\"store_true\")\n",
    "    parser.add_argument(\"--max_relations\", type=int, default=-1)\n",
    "    parser.add_argument(\"--graph-batch-size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--graph-split-size\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--negative-sample\", type=int, default=1)\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--evaluate-every\", type=int, default=10)\n",
    "    \n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--n-bases\", type=int, default=4)\n",
    "    \n",
    "#     parser.add_argument(\"--hid_dim\", type=int, default=768)\n",
    "    parser.add_argument(\"--hid_dim\", type=int, default=128)\n",
    "    parser.add_argument(\"--regularization\", type=float, default=1e-2)\n",
    "    parser.add_argument(\"--grad-norm\", type=float, default=1.0)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6a46b-221f-4db3-a189-608deb3a1785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba785c3d-74df-492c-8ade-83dfc132a104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdcdb9-5992-4e58-a893-90d788736ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490a908-b7ba-46a4-af1b-10a6bb9205ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (DPLClass environment)",
   "language": "python",
   "name": "dplclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
